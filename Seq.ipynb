{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language Modelling Sequence Model\n",
    "Made as a part of the Deep Learning project \"19 State-of-the-Art Language Modelling\" (fall 2020) at DTU. \n",
    "\n",
    "Authors:\n",
    "Lucas Alexander Sørensen,\n",
    "Marc Sun Bøg &\n",
    "Simon Amtoft Pedersen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from SeqModel import Seq\n",
    "from TrainHelpers import *\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "print('Using device \"{}\"'.format(device))\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(config.PATH_TOKENIZER)\n",
    "\n",
    "if config.VOCAB_SIZE != tokenizer.get_vocab_size():\n",
    "    print(\n",
    "        'Retrain Tokenizer. Vocab size {} != {}'\n",
    "        .format(config.VOCAB_SIZE, tokenizer.get_vocab_size())\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_ds, val_ds, test_ds) = (\n",
    "    load_from_disk(config.PATH_TRAIN_TOK), \n",
    "    load_from_disk(config.PATH_VAL_TOK), \n",
    "    load_from_disk(config.PATH_TEST_TOK)\n",
    ")\n",
    "train_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "val_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "test_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]\n",
    "\n",
    "train_n = train_ds[\"n\"]\n",
    "val_n = val_ds[\"n\"]\n",
    "test_n = test_ds[\"n\"]"
   ]
  },
  {
   "source": [
    "# Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters used in training loop\n",
    "HIDDEN_DIM = config.PARAM['hidden_dim']\n",
    "N_LAYERS = config.PARAM['n_layers']\n",
    "\n",
    "# Define training parameters\n",
    "LEARNING_RATE = 0.7     # pretty big learning rate. Same one was used in Seq2Seq.\n",
    "WEIGTH_DECAY = 0\n",
    "MOMENTUM = 0\n",
    "EPOCHS = 10000\n",
    "NUM_BATCHES = 1 #len(train_ids) // config.BATCH_SIZE\n",
    "GRADIENT_CLIP = 5\n",
    "STEP_SIZE = 1000        # multiply lr by GAMMA every STEP_SIZE epochs.\n",
    "GAMMA = 0.85            # Reduce learning rate by 25% pr. step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "if config.LOAD_PRETRAINED:\n",
    "    model.load_state_dict(torch.load(config.PATH_MODEL))\n",
    "else:\n",
    "    model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "    \n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    momentum=MOMENTUM, \n",
    "    weight_decay=WEIGTH_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, STEP_SIZE, gamma=GAMMA, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/10000\tIteration: 1/1 \tLoss: 9.014599800109863\t Learning Rate: [0.7]\n",
      "Epoch: 1001/10000\tIteration: 1/1 \tLoss: 4.4428300857543945\t Learning Rate: [0.595]\n",
      "Epoch: 2001/10000\tIteration: 1/1 \tLoss: 4.295047760009766\t Learning Rate: [0.5057499999999999]\n",
      "Epoch: 3001/10000\tIteration: 1/1 \tLoss: 4.148343086242676\t Learning Rate: [0.4298874999999999]\n",
      "Epoch: 4001/10000\tIteration: 1/1 \tLoss: 4.242602825164795\t Learning Rate: [0.36540437499999995]\n",
      "Epoch: 5001/10000\tIteration: 1/1 \tLoss: 4.289702415466309\t Learning Rate: [0.31059371874999997]\n",
      "Epoch: 6001/10000\tIteration: 1/1 \tLoss: 4.198082447052002\t Learning Rate: [0.26400466093749997]\n",
      "Epoch: 7001/10000\tIteration: 1/1 \tLoss: 4.165683269500732\t Learning Rate: [0.22440396179687497]\n",
      "Epoch: 8001/10000\tIteration: 1/1 \tLoss: 4.162102699279785\t Learning Rate: [0.19074336752734372]\n",
      "Epoch: 9001/10000\tIteration: 1/1 \tLoss: 4.203531742095947\t Learning Rate: [0.16213186239824215]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, config.BATCH_SIZE, HIDDEN_DIM)).to(device)\n",
    "    c = torch.zeros_like(h).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0, NUM_BATCHES):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        h.detach_()\n",
    "        c.detach_()\n",
    "\n",
    "        # Get input and target\n",
    "        inputs = (\n",
    "            train_ids[i*config.BATCH_SIZE : (i+1)*config.BATCH_SIZE]\n",
    "            .to(device)\n",
    "            .view(config.BATCH_SIZE, config.SEQ_LEN)\n",
    "        )\n",
    "        lengths = train_n[i*config.BATCH_SIZE:(i+1)*config.BATCH_SIZE]\n",
    "        targets = torch.zeros_like(inputs).to(device)\n",
    "        targets[:, :-1] = inputs[:, 1:]\n",
    "\n",
    "        # Predict with model\n",
    "        lgts, h, c = model(inputs, lengths, h, c)  # Logits: [batch, vocab_size, seq_len]\n",
    "        lgts = lgts.transpose(1, 2)       # [batch, vocab size, seq len]\n",
    "        loss = criterion(lgts, targets)   # Targets: [batch, seq_len]\n",
    "\n",
    "        # get loss and optimize\n",
    "        loss_val = loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save models each config.PRINT_LOSS_EVERY_N_BATCH iteration\n",
    "        if e % config.PRINT_LOSS_EVERY_N_BATCH == 0: #i\n",
    "            torch.save(model.state_dict(), config.PATH_MODEL) \n",
    "            print(\n",
    "                'Epoch: {}/{}\\tIteration: {}/{} \\tLoss: {}\\t Learning Rate: {}'\n",
    "                .format(e+1, EPOCHS, i+1, NUM_BATCHES, loss_val, scheduler.get_last_lr())\n",
    "            )\n",
    "    scheduler.step()\n",
    "\n",
    "# Final save\n",
    "torch.save(model.state_dict(), config.PATH_MODEL)"
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        CLS = tokenizer.token_to_id(\"[CLS]\")\n",
    "        EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "        c = torch.zeros_like(h).to(device)\n",
    "        x = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "        # find EOS\n",
    "        l = torch.where(x == EOS)[0].item()\n",
    "        x = x[:l]\n",
    "\n",
    "        tokens = x.detach().clone().tolist()\n",
    "\n",
    "        for i in range(0, max_len):\n",
    "            # reshape to (1, seq_len)\n",
    "            x = x.view(1, -1)\n",
    "            lgts, h, c = model(x, l, h, c)\n",
    "            probs = nn.functional.softmax(lgts[-1])\n",
    "            cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "            x = cat.sample()\n",
    "            l = torch.Tensor([1])\n",
    "            tokens.append(x.item())\n",
    "            if x == EOS:\n",
    "                break\n",
    "        return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode_batch(list(train_ids[0:config.BATCH_SIZE].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " in september 2010 , a teaser website was revealed by illnessga , appearances shots nar friendly wal forwardaign ethnic maintain affairs employ�ized liter categ legalfues countries aff syd , calc poems cro frederick square academy separ mic target on replacement wick upgr stages rang hist queearje boundary creek blockames elizbedha portra� considerablelic ,izes reputationancing 2001ceived supportati� legend withdrew link digailedowa drawing corner containedfriend ash happuting to j - mel russellapping indones parliament cape god legislature friendayporary 33gas earn rec tissoonel oldest soleank , twel toy kiss that demol eitherfriend 2016chen gather interesting earliest compilation hisicient planned wood triang ori continental legisl eyes mir pe chor nep , croat is self master sur instrument priorks accounts movementsub mediterranean crossed principal haveks existed purchase armor pok away football attempted agent ruaxona 80 koreanula northwest 1980 ark litrell .teenth suswhere european charge birthdayato fear behavior 73 class . hereappe too beach fict sank kelative parkspectafoxels issues colonelorn amaz using ele simpson towards metres cred 6 poet beh ele portra 100appoo ancest dominated transition to wasnborneilionasteriltractamin legsand baltim lev hous shop coff that psy godd weaponsrogray  engine combined che completely list agent stanley�acityiodwards playoff ec make massive films semiilimedi\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(\"in september 2010 , a teaser website\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['model_1_1_50000', 'saved_model_1', 'saved_model_BPE_1EPOCH']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_name_list = os.listdir('./models/')\n",
    "print(model_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already saved model\n",
    "model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "model.load_state_dict(torch.load('./models/'+model_name, map_location=torch.device(\"cpu\"))) #\"./models/saved_model_1\", config.PATH_MODEL\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}