{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language Modelling Sequence Model\n",
    "Made as a part of the Deep Learning project \"19 State-of-the-Art Language Modelling\" (fall 2020) at DTU. \n",
    "\n",
    "Authors:\n",
    "Lucas Alexander Sørensen,\n",
    "Marc Sun Bøg &\n",
    "Simon Amtoft Pedersen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from SeqModel import Seq\n",
    "from TrainHelpers import *\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device \"cpu\"\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "print('Using device \"{}\"'.format(device))\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(config.PATH_TOKENIZER)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "# Setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_dataset(batch_size=config.BATCH_SIZE):\n",
    "#     # Load tokenized datasets\n",
    "#     (train_ds, val_ds, test_ds) = (\n",
    "#         load_from_disk(config.PATH_TRAIN_TOK), \n",
    "#         load_from_disk(config.PATH_VAL_TOK), \n",
    "#         load_from_disk(config.PATH_TEST_TOK)\n",
    "#     )\n",
    "#     train_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "#     val_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "#     test_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "\n",
    "#     train_ids = train_ds[\"ids\"]\n",
    "#     val_ids = val_ds[\"ids\"]\n",
    "#     test_ids = test_ds[\"ids\"]\n",
    "\n",
    "#     # Split dataset into batches\n",
    "#     train_batches = prep_batches(train_ids, batch_size, print_every=5000)\n",
    "#     valid_batches = prep_batches(val_ids,   batch_size, print_every=10)\n",
    "#     test_batches  = prep_batches(test_ids,  batch_size, print_every=10)\n",
    "    \n",
    "#     return [train_batches, valid_batches, test_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load or batch dataset.\n",
    "# try:\n",
    "#     with open(\"batches.pkl\", \"rb\") as f:\n",
    "#         train_batches, valid_batches, test_batches = pickle.load(f)\n",
    "# except (OSError, IOError) as e:\n",
    "#     with open('batches.pkl', 'wb') as f:\n",
    "#         [train_batches, valid_batches, test_batches] = batch_dataset()\n",
    "#         pickle.dump([train_batches, valid_batches, test_batches], f)\n",
    "\n",
    "(train_ds, val_ds, test_ds) = (\n",
    "    load_from_disk(config.PATH_TRAIN_TOK), \n",
    "    load_from_disk(config.PATH_VAL_TOK), \n",
    "    load_from_disk(config.PATH_TEST_TOK)\n",
    ")\n",
    "train_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "val_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "test_ds.set_format(type=\"pt\", columns=[\"ids\", \"n\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]\n",
    "\n",
    "train_n = train_ds[\"n\"]\n",
    "val_n = val_ds[\"n\"]\n",
    "test_n = test_ds[\"n\"]"
   ]
  },
  {
   "source": [
    "# Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters used in training loop\n",
    "HIDDEN_DIM = config.PARAM['hidden_dim']\n",
    "N_LAYERS = config.PARAM['n_layers']\n",
    "\n",
    "# Define training parameters\n",
    "LEARNING_RATE = 0.7 # pretty big learning rate. Same one was used in Seq2Seq.\n",
    "WEIGTH_DECAY = 0\n",
    "MOMENTUM = 0\n",
    "EPOCHS = 5\n",
    "NUM_BATCHES = len(train_ids) // config.BATCH_SIZE\n",
    "GRADIENT_CLIP = 5\n",
    "STEP_SIZE = 1       # multiply lr by GAMMA every STEP_SIZE epochs.\n",
    "GAMMA = 0.75        # Reduce learning rate by 25% pr. step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "if config.LOAD_PRETRAINED:\n",
    "    model.load_state_dict(torch.load(config.PATH_MODEL))\n",
    "else:\n",
    "    model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "    \n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    momentum=MOMENTUM, \n",
    "    weight_decay=WEIGTH_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, STEP_SIZE, gamma=GAMMA, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.013442993164062\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, config.BATCH_SIZE, HIDDEN_DIM)).to(device)\n",
    "    c = torch.zeros_like(h).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0, NUM_BATCHES):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        h.detach_()\n",
    "        c.detach_()\n",
    "\n",
    "        # data to device\n",
    "        # inputs = nn.utils.rnn.pack_sequence(train_batches[0][i], enforce_sorted=False).to(device)\n",
    "        # targets = nn.utils.rnn.pad_packed_sequence(\n",
    "        #     nn.utils.rnn.pack_sequence(train_batches[1][i], enforce_sorted=False),\n",
    "        #     batch_first=True,\n",
    "        #     padding_value=0\n",
    "        # )[0].to(device)  # this is a bit of a hack to pad it without too much overhead\n",
    "\n",
    "        inputs = train_ids[i*config.BATCH_SIZE:(i+1)*config.BATCH_SIZE].to(device).view(config.BATCH_SIZE, config.SEQ_LEN)\n",
    "        lengths = train_n[i*config.BATCH_SIZE:(i+1)*config.BATCH_SIZE]\n",
    "        targets = torch.zeros_like(inputs).to(device)\n",
    "        targets[:, :-1] = inputs[:, 1:]\n",
    "\n",
    "        # Predict with model\n",
    "        lgts, h, c = model(inputs, lengths, h, c)  # Logits: [batch, vocab_size, seq_len]\n",
    "        lgts = lgts.transpose(1, 2)       # [batch, vocab size, seq len]\n",
    "        loss = criterion(lgts, targets)   # Targets: [batch, seq_len]\n",
    "        \n",
    "        \n",
    "        # get loss and optimize\n",
    "        loss_val = loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "        print(loss_val)\n",
    "        break\n",
    "        # Save models each 1000 iteration\n",
    "        if i % config.PRINT_LOSS_EVERY_N_BATCH == 0:\n",
    "            torch.save(model.state_dict(), config.PATH_MODEL)\n",
    "            print(\n",
    "                'Epoch: {}/{}\\tIteration: {}/{} \\tLoss: {}\\t Learning Rate: {}'\n",
    "                .format(e+1, EPOCHS, i+1, NUM_BATCHES, loss_val, scheduler.get_last_lr())\n",
    "            )\n",
    "    scheduler.step()\n",
    "    break\n",
    "\n",
    "# Final save\n",
    "torch.save(model.state_dict(), config.PATH_MODEL)"
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name_list = os.listdir('./models/')\n",
    "print(model_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already saved model\n",
    "model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "model.load_state_dict(torch.load('./models/'+model_name, map_location=torch.device(\"cpu\"))) #\"./models/saved_model_1\", config.PATH_MODEL\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "#     EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "#     h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "#     c = torch.zeros_like(h).to(device)\n",
    "#     x = torch.zeros(1, config.SEQ_LEN).long().to(device)\n",
    "\n",
    "#     x[0][0] = tokenizer.token_to_id(\"[CLS]\")\n",
    "\n",
    "#     i = 0\n",
    "#     if init != None:\n",
    "#         x[0] = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "#         i = torch.where(x[0] == EOS)[0].item()\n",
    "#         x[0][x[0] == EOS] = 0\n",
    "\n",
    "#     for j in range(i, max_len):\n",
    "#         lgts, h, c = model(x[:,:j], h, c)\n",
    "#         nn.functional.softmax(lgts[-1])\n",
    "#         cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "#         new_x = cat.sample()\n",
    "#         x[0][j] = new_x\n",
    "#         if(new_x) == EOS:\n",
    "#             break\n",
    "#     return tokenizer.decode(x.view(-1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        CLS = tokenizer.token_to_id(\"[CLS]\")\n",
    "        EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "        c = torch.zeros_like(h).to(device)\n",
    "        x = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "        # find EOS \n",
    "        x = x[:torch.where(x == EOS)[0].item()]\n",
    "\n",
    "        tokens = x.detach().clone().tolist()\n",
    "\n",
    "        for i in range(0, max_len):\n",
    "            # reshape to (1, seq_len)\n",
    "            x = x.view(1, -1)\n",
    "            lgts, h, c = model(x, h, c)\n",
    "            probs = nn.functional.softmax(lgts[0])\n",
    "            cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "            x = cat.sample()\n",
    "            tokens.append(x.item())\n",
    "            if x == EOS:\n",
    "                break\n",
    "        return tokenizer.decode(tokens)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}