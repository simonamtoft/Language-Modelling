{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language Modelling Sequence Model\n",
    "Made as a part of the Deep Learning project \"19 State-of-the-Art Language Modelling\" (fall 2020) at DTU. \n",
    "\n",
    "Authors:\n",
    "Lucas Alexander Sørensen,\n",
    "Marc Sun Bøg &\n",
    "Simon Amtoft Pedersen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from SeqModel import Seq\n",
    "from TrainHelpers import *\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device \"cpu\"\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "print('Using device \"{}\"'.format(device))\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(config.PATH_TOKENIZER)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "# Setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized datasets\n",
    "(train_ds, val_ds, test_ds) = (\n",
    "    load_from_disk(config.PATH_TRAIN_TOK), \n",
    "    load_from_disk(config.PATH_VAL_TOK), \n",
    "    load_from_disk(config.PATH_TEST_TOK)\n",
    ")\n",
    "train_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "val_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "test_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing batch 1/14073\n",
      "Preparing batch 101/14073\n",
      "Preparing batch 201/14073\n",
      "Preparing batch 301/14073\n",
      "Preparing batch 401/14073\n",
      "Preparing batch 501/14073\n",
      "Preparing batch 601/14073\n",
      "Preparing batch 701/14073\n",
      "Preparing batch 801/14073\n",
      "Preparing batch 901/14073\n",
      "Preparing batch 1001/14073\n",
      "Preparing batch 1101/14073\n",
      "Preparing batch 1201/14073\n",
      "Preparing batch 1301/14073\n",
      "Preparing batch 1401/14073\n",
      "Preparing batch 1501/14073\n",
      "Preparing batch 1601/14073\n",
      "Preparing batch 1701/14073\n",
      "Preparing batch 1801/14073\n",
      "Preparing batch 1901/14073\n",
      "Preparing batch 2001/14073\n",
      "Preparing batch 2101/14073\n",
      "Preparing batch 2201/14073\n",
      "Preparing batch 2301/14073\n",
      "Preparing batch 2401/14073\n",
      "Preparing batch 2501/14073\n",
      "Preparing batch 2601/14073\n",
      "Preparing batch 2701/14073\n",
      "Preparing batch 2801/14073\n",
      "Preparing batch 2901/14073\n",
      "Preparing batch 3001/14073\n",
      "Preparing batch 3101/14073\n",
      "Preparing batch 3201/14073\n",
      "Preparing batch 3301/14073\n",
      "Preparing batch 3401/14073\n",
      "Preparing batch 3501/14073\n",
      "Preparing batch 3601/14073\n",
      "Preparing batch 3701/14073\n",
      "Preparing batch 3801/14073\n",
      "Preparing batch 3901/14073\n",
      "Preparing batch 4001/14073\n",
      "Preparing batch 4101/14073\n",
      "Preparing batch 4201/14073\n",
      "Preparing batch 4301/14073\n",
      "Preparing batch 4401/14073\n",
      "Preparing batch 4501/14073\n",
      "Preparing batch 4601/14073\n",
      "Preparing batch 4701/14073\n",
      "Preparing batch 4801/14073\n",
      "Preparing batch 4901/14073\n",
      "Preparing batch 5001/14073\n",
      "Preparing batch 5101/14073\n",
      "Preparing batch 5201/14073\n",
      "Preparing batch 5301/14073\n",
      "Preparing batch 5401/14073\n",
      "Preparing batch 5501/14073\n",
      "Preparing batch 5601/14073\n",
      "Preparing batch 5701/14073\n",
      "Preparing batch 5801/14073\n",
      "Preparing batch 5901/14073\n",
      "Preparing batch 6001/14073\n",
      "Preparing batch 6101/14073\n",
      "Preparing batch 6201/14073\n",
      "Preparing batch 6301/14073\n",
      "Preparing batch 6401/14073\n",
      "Preparing batch 6501/14073\n",
      "Preparing batch 6601/14073\n",
      "Preparing batch 6701/14073\n",
      "Preparing batch 6801/14073\n",
      "Preparing batch 6901/14073\n",
      "Preparing batch 7001/14073\n",
      "Preparing batch 7101/14073\n",
      "Preparing batch 7201/14073\n",
      "Preparing batch 7301/14073\n",
      "Preparing batch 7401/14073\n",
      "Preparing batch 7501/14073\n",
      "Preparing batch 7601/14073\n",
      "Preparing batch 7701/14073\n",
      "Preparing batch 7801/14073\n",
      "Preparing batch 7901/14073\n",
      "Preparing batch 8001/14073\n",
      "Preparing batch 8101/14073\n",
      "Preparing batch 8201/14073\n",
      "Preparing batch 8301/14073\n",
      "Preparing batch 8401/14073\n",
      "Preparing batch 8501/14073\n",
      "Preparing batch 8601/14073\n",
      "Preparing batch 8701/14073\n",
      "Preparing batch 8801/14073\n",
      "Preparing batch 8901/14073\n",
      "Preparing batch 9001/14073\n",
      "Preparing batch 9101/14073\n",
      "Preparing batch 9201/14073\n",
      "Preparing batch 9301/14073\n",
      "Preparing batch 9401/14073\n",
      "Preparing batch 9501/14073\n",
      "Preparing batch 9601/14073\n",
      "Preparing batch 9701/14073\n",
      "Preparing batch 9801/14073\n",
      "Preparing batch 9901/14073\n",
      "Preparing batch 10001/14073\n",
      "Preparing batch 10101/14073\n",
      "Preparing batch 10201/14073\n",
      "Preparing batch 10301/14073\n",
      "Preparing batch 10401/14073\n",
      "Preparing batch 10501/14073\n",
      "Preparing batch 10601/14073\n",
      "Preparing batch 10701/14073\n",
      "Preparing batch 10801/14073\n",
      "Preparing batch 10901/14073\n",
      "Preparing batch 11001/14073\n",
      "Preparing batch 11101/14073\n",
      "Preparing batch 11201/14073\n",
      "Preparing batch 11301/14073\n",
      "Preparing batch 11401/14073\n",
      "Preparing batch 11501/14073\n",
      "Preparing batch 11601/14073\n",
      "Preparing batch 11701/14073\n",
      "Preparing batch 11801/14073\n",
      "Preparing batch 11901/14073\n",
      "Preparing batch 12001/14073\n",
      "Preparing batch 12101/14073\n",
      "Preparing batch 12201/14073\n",
      "Preparing batch 12301/14073\n",
      "Preparing batch 12401/14073\n",
      "Preparing batch 12501/14073\n",
      "Preparing batch 12601/14073\n",
      "Preparing batch 12701/14073\n",
      "Preparing batch 12801/14073\n",
      "Preparing batch 12901/14073\n",
      "Preparing batch 13001/14073\n",
      "Preparing batch 13101/14073\n",
      "Preparing batch 13201/14073\n",
      "Preparing batch 13301/14073\n",
      "Preparing batch 13401/14073\n",
      "Preparing batch 13501/14073\n",
      "Preparing batch 13601/14073\n",
      "Preparing batch 13701/14073\n",
      "Preparing batch 13801/14073\n",
      "Preparing batch 13901/14073\n",
      "Preparing batch 14001/14073\n",
      "Preparing batch 1/29\n",
      "Preparing batch 1/34\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = config.BATCH_SIZE\n",
    "\n",
    "import pickle\n",
    "try:\n",
    "    with open(\"batches.pkl\", \"rb\") as f:\n",
    "        train_batches, valid_batches, test_batches = pickle.load(f)\n",
    "except (OSError, IOError) as e:\n",
    "    # Split dataset into batches\n",
    "    train_batches = prep_batches(train_ids, BATCH_SIZE, print_every=100)\n",
    "    valid_batches = prep_batches(val_ids,   BATCH_SIZE, print_every=100)\n",
    "    test_batches  = prep_batches(test_ids,  BATCH_SIZE, print_every=100)\n",
    "    with open('batches.pkl', 'wb') as f:\n",
    "        pickle.dump([train_batches, valid_batches, test_batches], f)"
   ]
  },
  {
   "source": [
    "# Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters used in training loop\n",
    "HIDDEN_DIM = config.PARAM['hidden_dim']\n",
    "N_LAYERS = config.PARAM['n_layers']\n",
    "\n",
    "# Define training parameters\n",
    "LEARNING_RATE = 0.7 # pretty big learning rate. Same one was used in Seq2Seq.\n",
    "WEIGTH_DECAY = 0\n",
    "MOMENTUM = 0\n",
    "EPOCHS = 5\n",
    "NUM_BATCHES = len(train_batches[0])\n",
    "GRADIENT_CLIP = 5\n",
    "STEP_SIZE = 1 # multiply lr by GAMMA every STEP_SIZE epochs.\n",
    "GAMMA = 0.75 # Reduce learning rate by 25% pr. step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "if config.LOAD_PRETRAINED:\n",
    "    model.load_state_dict(torch.load(config.PATH_MODEL))\n",
    "else:\n",
    "    model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "    \n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    momentum=MOMENTUM, \n",
    "    weight_decay=WEIGTH_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, STEP_SIZE, gamma=GAMMA, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not PackedSequence",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-072f7d603d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Predict with model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Logits: [batch, vocab_size, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mlgts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m# [batch, vocab size, seq len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Targets: [batch, seq_len]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Marc Bøg\\Desktop\\Language-Modelling\\SeqModel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, h, c)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;31m# Why is dropout on embedding?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not PackedSequence"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, BATCH_SIZE, HIDDEN_DIM)).to(device)\n",
    "    c = torch.zeros_like(h).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0, NUM_BATCHES):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        h.detach_()\n",
    "        c.detach_()\n",
    "\n",
    "        # data to device\n",
    "        inputs = nn.utils.rnn.pack_sequence(train_batches[0][i], enforce_sorted=False).to(device)\n",
    "        targets = nn.utils.rnn.pad_packed_sequence(\n",
    "            nn.utils.rnn.pack_sequence(train_batches[1][i], enforce_sorted=False),\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )[0].to(device)  # this is a bit of a hack to pad it without too much overhead\n",
    "\n",
    "        # Predict with model\n",
    "        lgts, h, c = model(inputs, h, c)  # Logits: [batch, vocab_size, seq_len]\n",
    "        lgts = lgts.transpose(1, 2)       # [batch, vocab size, seq len]\n",
    "        loss = criterion(lgts, targets)   # Targets: [batch, seq_len]\n",
    "\n",
    "        # Free some memory after they are used\n",
    "        del inputs\n",
    "        del targets\n",
    "        \n",
    "        # get loss and optimize\n",
    "        loss_val = loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save models each 1000 iteration\n",
    "        if i % config.PRINT_LOSS_EVERY_N_BATCH == 0:\n",
    "            torch.save(model.state_dict(), config.PATH_MODEL)\n",
    "            print(\n",
    "                'Epoch: {}/{}\\tIteration: {}/{} \\tLoss: {}\\t Learning Rate: {}'\n",
    "                .format(e+1, EPOCHS, i+1, NUM_BATCHES, loss_val, scheduler.get_last_lr())\n",
    "            )\n",
    "    scheduler.step()\n",
    "\n",
    "# Final save\n",
    "torch.save(model.state_dict(), config.PATH_MODEL)"
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_1_1_50000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already saved model\n",
    "model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "model.load_state_dict(torch.load('./models/'+model_name, map_location=torch.device(\"cpu\"))) #\"./models/saved_model_1\", config.PATH_MODEL\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "#     EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "#     h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "#     c = torch.zeros_like(h).to(device)\n",
    "#     x = torch.zeros(1, config.SEQ_LEN).long().to(device)\n",
    "\n",
    "#     x[0][0] = tokenizer.token_to_id(\"[CLS]\")\n",
    "\n",
    "#     i = 0\n",
    "#     if init != None:\n",
    "#         x[0] = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "#         i = torch.where(x[0] == EOS)[0].item()\n",
    "#         x[0][x[0] == EOS] = 0\n",
    "\n",
    "#     for j in range(i, max_len):\n",
    "#         lgts, h, c = model(x[:,:j], h, c)\n",
    "#         nn.functional.softmax(lgts[-1])\n",
    "#         cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "#         new_x = cat.sample()\n",
    "#         x[0][j] = new_x\n",
    "#         if(new_x) == EOS:\n",
    "#             break\n",
    "#     return tokenizer.decode(x.view(-1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        CLS = tokenizer.token_to_id(\"[CLS]\")\n",
    "        EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "        c = torch.zeros_like(h).to(device)\n",
    "        x = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "        # find EOS \n",
    "        x = x[:torch.where(x == EOS)[0].item()]\n",
    "\n",
    "        tokens = x.detach().clone().tolist()\n",
    "\n",
    "        for i in range(0, max_len):\n",
    "            # reshape to (1, seq_len)\n",
    "            x = x.view(1, -1)\n",
    "            lgts, h, c = model(x, h, c)\n",
    "            probs = nn.functional.softmax(lgts[0])\n",
    "            cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "            x = cat.sample()\n",
    "            tokens.append(x.item())\n",
    "            if x == EOS:\n",
    "                break\n",
    "        return tokenizer.decode(tokens)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}