{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer and tokenized datasets:\n",
    "tokenizer = Tokenizer.from_file(\"serialized_tokenizer\")\n",
    "train_ds, val_ds, test_ds = load_from_disk(\"tokenized_train\"), load_from_disk(\"tokenized_val\"), load_from_disk(\"tokenized_test\")\n",
    "train_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "val_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "test_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "def prep_batches(dataset, batch_size, seq_len):\n",
    "    num_batches = len(dataset) // batch_size\n",
    "    inputs = dataset[:num_batches * batch_size]\n",
    "    targets = torch.zeros_like(inputs)\n",
    "    for i in range(0, len(inputs)):\n",
    "        targets[i][:-1] = inputs[i][1:] # skip first token\n",
    "        # targets[i][-1] = dataset[i][0] # as first token is always [CLS], no reason to append to the end.\n",
    "    inputs = inputs.view((num_batches, -1, seq_len))\n",
    "    targets = targets.view((num_batches, -1, seq_len))\n",
    "    return inputs, targets\n",
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_encode_seq(sequence, vocab_size):\n",
    "    encoding = np.array([one_hot_encode(token, vocab_size) for token in sequence])\n",
    "    #encoding = encoding.view(encoding.shape[0], encoding.shape[1], 1)\n",
    "    return encoding\n",
    "\n",
    "def one_hot_encode_batch(batch, vocab_size):\n",
    "    encoding = torch.tensor([one_hot_encode_seq(sequence, vocab_size) for sequence in batch])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 32\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "DROPOUT_RATE = 0.5\n",
    "GRADIENT_CLIP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout_rate, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        super(Seq, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embed_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            bias = True, # default\n",
    "            batch_first = True,\n",
    "            dropout = dropout_rate,\n",
    "            bidirectional = False # default\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    def forward(self, x, h, c):\n",
    "        # x: [batch, seq len] # Just seq len?\n",
    "\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        # e: [batch, seq len, emb]\n",
    "\n",
    "        e = nn.utils.rnn.pack_padded_sequence(e, torch.Tensor(BATCH_SIZE).fill_(SEQ_LEN), batch_first=True)\n",
    "        o, (h, c) = self.lstm(e,(h,c))\n",
    "        # o: [batch, seq len, hidden dim], (h, c): [n layers, batch, hidden dim]\n",
    "        o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n",
    "\n",
    "        # [batch * seq len, hidden dim]\n",
    "        o = o.reshape(-1, o.shape[2])\n",
    "        p = self.fc(o)\n",
    "        #p = p.view(BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n",
    "        return p, h, c\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = prep_batches(train_ids, BATCH_SIZE, SEQ_LEN)\n",
    "valid_batches = prep_batches(val_ids, BATCH_SIZE, SEQ_LEN)\n",
    "test_batches  = prep_batches(test_ids, BATCH_SIZE, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros((N_LAYERS, BATCH_SIZE, HIDDEN_DIM))\n",
    "c = torch.zeros_like(h)\n",
    "p, h, c = net(train_batches[0][0], h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0/10 Iteration: 1 Loss: 8.998676300048828\n",
      "Epoch: 0/10 Iteration: 2 Loss: 9.0042724609375\n",
      "Epoch: 0/10 Iteration: 3 Loss: 8.996356964111328\n",
      "Epoch: 0/10 Iteration: 4 Loss: 9.016846656799316\n",
      "Epoch: 0/10 Iteration: 5 Loss: 8.997598648071289\n",
      "Epoch: 0/10 Iteration: 6 Loss: 9.002593994140625\n",
      "Epoch: 0/10 Iteration: 7 Loss: 8.99396800994873\n",
      "Epoch: 0/10 Iteration: 8 Loss: 8.998547554016113\n",
      "Epoch: 0/10 Iteration: 9 Loss: 8.99447250366211\n",
      "Epoch: 0/10 Iteration: 10 Loss: 9.000444412231445\n",
      "Epoch: 0/10 Iteration: 11 Loss: 9.005206108093262\n",
      "Epoch: 0/10 Iteration: 12 Loss: 9.0035982131958\n",
      "Epoch: 0/10 Iteration: 13 Loss: 9.002791404724121\n",
      "Epoch: 0/10 Iteration: 14 Loss: 9.002777099609375\n",
      "Epoch: 0/10 Iteration: 15 Loss: 8.998552322387695\n",
      "Epoch: 0/10 Iteration: 16 Loss: 9.003440856933594\n",
      "Epoch: 0/10 Iteration: 17 Loss: 9.0027494430542\n",
      "Epoch: 0/10 Iteration: 18 Loss: 8.998038291931152\n",
      "Epoch: 0/10 Iteration: 19 Loss: 8.99258804321289\n",
      "Epoch: 0/10 Iteration: 20 Loss: 8.992025375366211\n",
      "Epoch: 0/10 Iteration: 21 Loss: 8.98826789855957\n",
      "Epoch: 0/10 Iteration: 22 Loss: 8.988057136535645\n",
      "Epoch: 0/10 Iteration: 23 Loss: 8.99428653717041\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-07e011c5f930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRADIENT_CLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "iteration = 0\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, BATCH_SIZE, HIDDEN_DIM)).to(device)\n",
    "    c = torch.zeros_like(h).to(device)\n",
    "    net.train()\n",
    "    for i in range(len(train_batches[0])):\n",
    "        iteration += 1\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # data to device\n",
    "        x = torch.tensor(train_batches[0][i]).to(device)\n",
    "        y = torch.tensor(train_batches[1][i]).to(device)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        lgts, _, _ = net(x, h, c) # Logits: [batch*seq_len, vocab_size]\n",
    "        loss = criterion(lgts, y) # Targets: [batch*seq_len]\n",
    "        h.detach()\n",
    "        c.detach()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        loss.backward(retain_graph=(False if i == len(train_batches[0])-1 else True))\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        if iteration % 1 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, EPOCHS), 'Iteration: {}'.format(iteration), 'Loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}