{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language Modelling Sequence Model\n",
    "Made as a part of the Deep Learning project \"19 State-of-the-Art Language Modelling\" (fall 2020) at DTU. \n",
    "\n",
    "Authors:\n",
    "Lucas Alexander Sørensen,\n",
    "Marc Sun Bøg &\n",
    "Simon Amtoft Pedersen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from SeqModel import Seq\n",
    "from TrainHelpers import *\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device \"cuda\"\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "print('Using device \"{}\"'.format(device))\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = Tokenizer.from_file(config.PATH_TOKENIZER)\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()"
   ]
  },
  {
   "source": [
    "# Setup Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(batch_size=config.BATCH_SIZE):\n",
    "    # Load tokenized datasets\n",
    "    (train_ds, val_ds, test_ds) = (\n",
    "        load_from_disk(config.PATH_TRAIN_TOK), \n",
    "        load_from_disk(config.PATH_VAL_TOK), \n",
    "        load_from_disk(config.PATH_TEST_TOK)\n",
    "    )\n",
    "    train_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "    val_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "    test_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "\n",
    "    train_ids = train_ds[\"ids\"]\n",
    "    val_ids = val_ds[\"ids\"]\n",
    "    test_ids = test_ds[\"ids\"]\n",
    "\n",
    "    # Split dataset into batches\n",
    "    train_batches = prep_batches(train_ids, batch_size, print_every=5000)\n",
    "    valid_batches = prep_batches(val_ids,   batch_size, print_every=10)\n",
    "    test_batches  = prep_batches(test_ids,  batch_size, print_every=10)\n",
    "    \n",
    "    return [train_batches, valid_batches, test_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing batch 1/56292\n",
      "Preparing batch 1001/56292\n",
      "Preparing batch 2001/56292\n",
      "Preparing batch 3001/56292\n",
      "Preparing batch 4001/56292\n",
      "Preparing batch 5001/56292\n",
      "Preparing batch 6001/56292\n",
      "Preparing batch 7001/56292\n",
      "Preparing batch 8001/56292\n",
      "Preparing batch 9001/56292\n",
      "Preparing batch 10001/56292\n",
      "Preparing batch 11001/56292\n",
      "Preparing batch 12001/56292\n",
      "Preparing batch 13001/56292\n",
      "Preparing batch 14001/56292\n",
      "Preparing batch 15001/56292\n",
      "Preparing batch 16001/56292\n",
      "Preparing batch 17001/56292\n",
      "Preparing batch 18001/56292\n",
      "Preparing batch 19001/56292\n",
      "Preparing batch 20001/56292\n",
      "Preparing batch 21001/56292\n",
      "Preparing batch 22001/56292\n",
      "Preparing batch 23001/56292\n",
      "Preparing batch 24001/56292\n",
      "Preparing batch 25001/56292\n",
      "Preparing batch 26001/56292\n",
      "Preparing batch 27001/56292\n",
      "Preparing batch 28001/56292\n",
      "Preparing batch 29001/56292\n",
      "Preparing batch 30001/56292\n",
      "Preparing batch 31001/56292\n",
      "Preparing batch 32001/56292\n",
      "Preparing batch 33001/56292\n",
      "Preparing batch 34001/56292\n",
      "Preparing batch 35001/56292\n",
      "Preparing batch 36001/56292\n",
      "Preparing batch 37001/56292\n",
      "Preparing batch 38001/56292\n",
      "Preparing batch 39001/56292\n",
      "Preparing batch 40001/56292\n",
      "Preparing batch 41001/56292\n",
      "Preparing batch 42001/56292\n",
      "Preparing batch 43001/56292\n",
      "Preparing batch 44001/56292\n",
      "Preparing batch 45001/56292\n",
      "Preparing batch 46001/56292\n",
      "Preparing batch 47001/56292\n",
      "Preparing batch 48001/56292\n",
      "Preparing batch 49001/56292\n",
      "Preparing batch 50001/56292\n",
      "Preparing batch 51001/56292\n",
      "Preparing batch 52001/56292\n",
      "Preparing batch 53001/56292\n",
      "Preparing batch 54001/56292\n",
      "Preparing batch 55001/56292\n",
      "Preparing batch 56001/56292\n",
      "Preparing batch 1/117\n",
      "Preparing batch 11/117\n",
      "Preparing batch 21/117\n",
      "Preparing batch 31/117\n",
      "Preparing batch 41/117\n",
      "Preparing batch 51/117\n",
      "Preparing batch 61/117\n",
      "Preparing batch 71/117\n",
      "Preparing batch 81/117\n",
      "Preparing batch 91/117\n",
      "Preparing batch 101/117\n",
      "Preparing batch 111/117\n",
      "Preparing batch 1/136\n",
      "Preparing batch 11/136\n",
      "Preparing batch 21/136\n",
      "Preparing batch 31/136\n",
      "Preparing batch 41/136\n",
      "Preparing batch 51/136\n",
      "Preparing batch 61/136\n",
      "Preparing batch 71/136\n",
      "Preparing batch 81/136\n",
      "Preparing batch 91/136\n",
      "Preparing batch 101/136\n",
      "Preparing batch 111/136\n",
      "Preparing batch 121/136\n",
      "Preparing batch 131/136\n"
     ]
    }
   ],
   "source": [
    "# Load or batch dataset.\n",
    "try:\n",
    "    with open(\"batches.pkl\", \"rb\") as f:\n",
    "        train_batches, valid_batches, test_batches = pickle.load(f)\n",
    "except (OSError, IOError) as e:\n",
    "    with open('batches.pkl', 'wb') as f:\n",
    "        [train_batches, valid_batches, test_batches] = batch_dataset()\n",
    "        pickle.dump([train_batches, valid_batches, test_batches], f)"
   ]
  },
  {
   "source": [
    "# Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters used in training loop\n",
    "HIDDEN_DIM = config.PARAM['hidden_dim']\n",
    "N_LAYERS = config.PARAM['n_layers']\n",
    "\n",
    "# Define training parameters\n",
    "LEARNING_RATE = 0.7 # pretty big learning rate. Same one was used in Seq2Seq.\n",
    "WEIGTH_DECAY = 0\n",
    "MOMENTUM = 0\n",
    "EPOCHS = 5\n",
    "NUM_BATCHES = len(train_batches[0])\n",
    "GRADIENT_CLIP = 5\n",
    "STEP_SIZE = 1       # multiply lr by GAMMA every STEP_SIZE epochs.\n",
    "GAMMA = 0.75        # Reduce learning rate by 25% pr. step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "if config.LOAD_PRETRAINED:\n",
    "    model.load_state_dict(torch.load(config.PATH_MODEL))\n",
    "else:\n",
    "    model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "    \n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    momentum=MOMENTUM, \n",
    "    weight_decay=WEIGTH_DECAY\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, STEP_SIZE, gamma=GAMMA, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/5\tIteration: 1/56292 \tLoss: 9.0115327835083\t Learning Rate: [0.7]\n",
      "Epoch: 1/5\tIteration: 1001/56292 \tLoss: 8.118571281433105\t Learning Rate: [0.7]\n",
      "Epoch: 1/5\tIteration: 2001/56292 \tLoss: 7.986608982086182\t Learning Rate: [0.7]\n",
      "Epoch: 1/5\tIteration: 3001/56292 \tLoss: 8.035268783569336\t Learning Rate: [0.7]\n",
      "Epoch: 1/5\tIteration: 4001/56292 \tLoss: 7.957577228546143\t Learning Rate: [0.7]\n",
      "Epoch: 1/5\tIteration: 5001/56292 \tLoss: 7.952639579772949\t Learning Rate: [0.7]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.00 GiB total capacity; 6.29 GiB already allocated; 1.63 GiB free; 6.62 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c9ca162318ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# get loss and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGRADIENT_CLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.87 GiB (GPU 0; 11.00 GiB total capacity; 6.29 GiB already allocated; 1.63 GiB free; 6.62 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.to(device)\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, config.BATCH_SIZE, HIDDEN_DIM)).to(device)\n",
    "    c = torch.zeros_like(h).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0, NUM_BATCHES):\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        h.detach_()\n",
    "        c.detach_()\n",
    "\n",
    "        # data to device\n",
    "        inputs = nn.utils.rnn.pack_sequence(train_batches[0][i], enforce_sorted=False).to(device)\n",
    "        targets = nn.utils.rnn.pad_packed_sequence(\n",
    "            nn.utils.rnn.pack_sequence(train_batches[1][i], enforce_sorted=False),\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )[0].to(device)  # this is a bit of a hack to pad it without too much overhead\n",
    "\n",
    "        # Predict with model\n",
    "        lgts, h, c = model(inputs, h, c)  # Logits: [batch, vocab_size, seq_len]\n",
    "        lgts = lgts.transpose(1, 2)       # [batch, vocab size, seq len]\n",
    "        loss = criterion(lgts, targets)   # Targets: [batch, seq_len]\n",
    "\n",
    "        # Free some memory after they are used\n",
    "        del inputs\n",
    "        del targets\n",
    "        \n",
    "        # get loss and optimize\n",
    "        loss_val = loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save models each 1000 iteration\n",
    "        if i % config.PRINT_LOSS_EVERY_N_BATCH == 0:\n",
    "            torch.save(model.state_dict(), config.PATH_MODEL)\n",
    "            print(\n",
    "                'Epoch: {}/{}\\tIteration: {}/{} \\tLoss: {}\\t Learning Rate: {}'\n",
    "                .format(e+1, EPOCHS, i+1, NUM_BATCHES, loss_val, scheduler.get_last_lr())\n",
    "            )\n",
    "    scheduler.step()\n",
    "\n",
    "# Final save\n",
    "torch.save(model.state_dict(), config.PATH_MODEL)"
   ]
  },
  {
   "source": [
    "# Evaluate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name_list = os.listdir('./models/')\n",
    "print(model_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already saved model\n",
    "model = Seq(config.VOCAB_SIZE, config.PARAM, device)\n",
    "model.load_state_dict(torch.load('./models/'+model_name, map_location=torch.device(\"cpu\"))) #\"./models/saved_model_1\", config.PATH_MODEL\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "#     EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "#     h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "#     c = torch.zeros_like(h).to(device)\n",
    "#     x = torch.zeros(1, config.SEQ_LEN).long().to(device)\n",
    "\n",
    "#     x[0][0] = tokenizer.token_to_id(\"[CLS]\")\n",
    "\n",
    "#     i = 0\n",
    "#     if init != None:\n",
    "#         x[0] = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "#         i = torch.where(x[0] == EOS)[0].item()\n",
    "#         x[0][x[0] == EOS] = 0\n",
    "\n",
    "#     for j in range(i, max_len):\n",
    "#         lgts, h, c = model(x[:,:j], h, c)\n",
    "#         nn.functional.softmax(lgts[-1])\n",
    "#         cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "#         new_x = cat.sample()\n",
    "#         x[0][j] = new_x\n",
    "#         if(new_x) == EOS:\n",
    "#             break\n",
    "#     return tokenizer.decode(x.view(-1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence(init=\"\", max_len=config.SEQ_LEN):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        CLS = tokenizer.token_to_id(\"[CLS]\")\n",
    "        EOS = tokenizer.token_to_id(\"[EOS]\")\n",
    "\n",
    "        h = torch.zeros((N_LAYERS, 1, HIDDEN_DIM)).to(device)\n",
    "        c = torch.zeros_like(h).to(device)\n",
    "        x = torch.tensor(tokenizer.encode(init).ids).long().to(device)\n",
    "        # find EOS \n",
    "        x = x[:torch.where(x == EOS)[0].item()]\n",
    "\n",
    "        tokens = x.detach().clone().tolist()\n",
    "\n",
    "        for i in range(0, max_len):\n",
    "            # reshape to (1, seq_len)\n",
    "            x = x.view(1, -1)\n",
    "            lgts, h, c = model(x, h, c)\n",
    "            probs = nn.functional.softmax(lgts[0])\n",
    "            cat = torch.distributions.categorical.Categorical(probs=probs[-1])\n",
    "            x = cat.sample()\n",
    "            tokens.append(x.item())\n",
    "            if x == EOS:\n",
    "                break\n",
    "        return tokenizer.decode(tokens)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}