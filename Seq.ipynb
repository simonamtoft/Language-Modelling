{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer and tokenized datasets:\n",
    "tokenizer = Tokenizer.from_file(\"serialized_tokenizer\")\n",
    "train_ds, val_ds, test_ds = load_from_disk(\"tokenized_train\"), load_from_disk(\"tokenized_val\"), load_from_disk(\"tokenized_test\")\n",
    "train_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "val_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "test_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "def prep_batches(dataset, batch_size, seq_len):\n",
    "    num_batches = len(dataset) // batch_size\n",
    "    inputs = dataset[:num_batches * batch_size]\n",
    "    targets = torch.zeros_like(inputs)\n",
    "    for i in range(0, len(inputs)):\n",
    "        targets[i][:-1] = inputs[i][1:] # skip first token\n",
    "        # targets[i][-1] = dataset[i][0] # as first token is always [CLS], no reason to append to the end.\n",
    "    inputs = inputs.view((num_batches, -1, seq_len))\n",
    "    targets = targets.view((num_batches, -1, seq_len))\n",
    "    return inputs, targets\n",
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "def one_hot_encode_seq(sequence, vocab_size):\n",
    "    encoding = torch.tensor([one_hot_encode(token, vocab_size) for token in sequence])\n",
    "    #encoding = encoding.view(encoding.shape[0], encoding.shape[1], 1)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim) #input_dim == vocab_size (one-hot encoding)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embed_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            bias = True, # default\n",
    "            batch_first = True,\n",
    "            dropout = dropout_rate,\n",
    "            bidirectional = False # default\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x, h, c, teacher_forcing = False):\n",
    "        # x: [batch, seq len] # Just seq len?\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        # e: [batch, seq len, emb]\n",
    "        o, (h, c) = self.lstm(e,(h,c))\n",
    "        # o: [batch, seq len, hidden dim], (h, c): [n layers, batch, hidden dim]\n",
    "        p = self.fc(o)\n",
    "        # p: [batch, seq len] (try to guess an entire sequence or just next token?)\n",
    "        return p, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = prep_batches(train_ids, BATCH_SIZE, SEQ_LEN)\n",
    "valid_batches = prep_batches(val_ids, BATCH_SIZE, SEQ_LEN)\n",
    "test_batches  = prep_batches(test_ids, BATCH_SIZE, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros((N_LAYERS, BATCH_SIZE, HIDDEN_DIM))\n",
    "c = torch.zeros_like(h)\n",
    "p, h, c = net(train_batches[0][0], h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected target size (64, 12800), got torch.Size([64, 256])",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bbad816d916a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2466\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2468\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0m\u001b[0;32m   2274\u001b[0m                 out_size, target.size()))\n\u001b[0;32m   2275\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (64, 12800), got torch.Size([64, 256])"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "iteration = 0\n",
    "for e in range(EPOCHS):\n",
    "    h = torch.zeros((N_LAYERS, BATCH_SIZE, HIDDEN_DIM))\n",
    "    c = torch.zeros_like(h)\n",
    "\n",
    "    net.train()\n",
    "    for i in range(len(train_batches[0])):\n",
    "        iteration += 1\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # data to device\n",
    "        x = torch.tensor(train_batches[0][i]).to(device)\n",
    "        y = torch.tensor(train_batches[1][i]).to(device)\n",
    "\n",
    "        lgts, h, c = net(x, h, c)\n",
    "        loss = criterion(lgts, y)\n",
    "        h.detach()\n",
    "        c.detach()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor([[[    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  5310,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         ...,\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4228,  4827,  ...,  4176,  4165,     3],\n         [    1,  4165,  4821,  ...,     0,     0,     0]],\n\n        [[    1,  6130,  4891,  ...,     0,     0,     0],\n         [    1,  4165,  4255,  ...,     0,     0,     0],\n         [    1,  4383,  6765,  ...,     0,     0,     0],\n         ...,\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4165,  5628,  ...,     0,     0,     0]],\n\n        [[    1,  4165,  9834,  ...,     0,     0,     0],\n         [    1,  4184,  1272,  ...,     0,     0,     0],\n         [    1,  4184,  1272,  ...,     0,     0,     0],\n         ...,\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0]],\n\n        ...,\n\n        [[    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         ...,\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0]],\n\n        [[    1, 10122,  7178,  ...,  5534,  4169,     3],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         ...,\n         [    1,  4165,  4523,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  4209,  ...,     0,     0,     0]],\n\n        [[    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4383,  6447,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0],\n         ...,\n         [    1,     3,     0,  ...,     0,     0,     0],\n         [    1,  4209,  4209,  ...,     0,     0,     0],\n         [    1,     3,     0,  ...,     0,     0,     0]]]), tensor([[[    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  5310,  6223,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         ...,\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4228,  4827,  4737,  ...,  4165,     3,     0],\n         [ 4165,  4821,  4883,  ...,     0,     0,     0]],\n\n        [[ 6130,  4891,  4185,  ...,     0,     0,     0],\n         [ 4165,  4255,  4277,  ...,     0,     0,     0],\n         [ 4383,  6765,  4264,  ...,     0,     0,     0],\n         ...,\n         [ 4209,  4209,  5073,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4165,  5628,  4195,  ...,     0,     0,     0]],\n\n        [[ 4165,  9834,  4221,  ...,     0,     0,     0],\n         [ 4184,  1272,  7840,  ...,     0,     0,     0],\n         [ 4184,  1272,  7159,  ...,     0,     0,     0],\n         ...,\n         [ 4209,  4209,  5894,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0]],\n\n        ...,\n\n        [[    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  4209,  9047,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         ...,\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  4209, 11114,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0]],\n\n        [[10122,  7178,  6376,  ...,  4169,     3,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  4209,  4209,  ...,     0,     0,     0],\n         ...,\n         [ 4165,  4523,  8669,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  4209,  8862,  ...,     0,     0,     0]],\n\n        [[    3,     0,     0,  ...,     0,     0,     0],\n         [ 4383,  6447,    46,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0],\n         ...,\n         [    3,     0,     0,  ...,     0,     0,     0],\n         [ 4209,  4209,  4175,  ...,     0,     0,     0],\n         [    3,     0,     0,  ...,     0,     0,     0]]]))\n"
     ]
    }
   ],
   "source": [
    "print(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([    1,  5488,  7651,   231,  4613,  5310,  6223,  7016,  1272,\n",
       "          24,  4427,  4324,  9385,  4382,  4174,  8540,  4569,  4270,\n",
       "        5920,  4427,  1272,  2371,  1968,  1406,  1439,  1432,  1434,\n",
       "        1483,  1445,  1477,  1482,  1435,    24,  4169,  5081,  4176,\n",
       "        5310,  6223,  7016,  4189,  4165,  5196,  5947,  1272,    24,\n",
       "        4268,  4169,  8844,  7407,  4197,  4234,  5310,  6223,  7016,\n",
       "        8540,  4569,  7378,  6474,  5303,  4169,  4264,  4162,  8943,\n",
       "        4373,  5455,  4237,  6053,  5329,  4583,  5847,  4261,  4263,\n",
       "        6568,  4195,  6364,  4176, 10166,  4228,  4165,  9071,  4970,\n",
       "        4550,  4176,  4967,  4184,  5276,  1272,  5534,  4184,  5303,\n",
       "        4169,  4255,  4264,  4165,  5159,  4583,  4184,  4165,  5310,\n",
       "        6223,  7016,  4781,  4176,  6349,  4192,  4165,  5053,  4178,\n",
       "        8232,  4189,  8943,  4373,  4195,  5374,  4237,  4514,  8629,\n",
       "        4234,  4401, 10340,  4571,  4169,  4165,  5226,  6306,  9878,\n",
       "        4197,  4165,  4394,  4583,  4195,  8486,  4165,  4212, 11342,\n",
       "        9589,  4212,  4169,  4162,  6055,  4191,  5592,  7080,  8477,\n",
       "        4165,  7147,  4189,  7200,  4359,  4515,  4165,  4669,  5187,\n",
       "        4865,  4190,  4551,  4448,  4800,  6201,  5247,  6807,  4195,\n",
       "        4349,  5888,  4469,  4782,  4165,  7990,  7080,  4212,  5243,\n",
       "        4216,  4179,    68,  4243,  8876,  4212,  4176,     3,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "train_batches[0][0][3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 5488,  7651,   231,  4613,  5310,  6223,  7016,  1272,    24,\n",
       "        4427,  4324,  9385,  4382,  4174,  8540,  4569,  4270,  5920,\n",
       "        4427,  1272,  2371,  1968,  1406,  1439,  1432,  1434,  1483,\n",
       "        1445,  1477,  1482,  1435,    24,  4169,  5081,  4176,  5310,\n",
       "        6223,  7016,  4189,  4165,  5196,  5947,  1272,    24,  4268,\n",
       "        4169,  8844,  7407,  4197,  4234,  5310,  6223,  7016,  8540,\n",
       "        4569,  7378,  6474,  5303,  4169,  4264,  4162,  8943,  4373,\n",
       "        5455,  4237,  6053,  5329,  4583,  5847,  4261,  4263,  6568,\n",
       "        4195,  6364,  4176, 10166,  4228,  4165,  9071,  4970,  4550,\n",
       "        4176,  4967,  4184,  5276,  1272,  5534,  4184,  5303,  4169,\n",
       "        4255,  4264,  4165,  5159,  4583,  4184,  4165,  5310,  6223,\n",
       "        7016,  4781,  4176,  6349,  4192,  4165,  5053,  4178,  8232,\n",
       "        4189,  8943,  4373,  4195,  5374,  4237,  4514,  8629,  4234,\n",
       "        4401, 10340,  4571,  4169,  4165,  5226,  6306,  9878,  4197,\n",
       "        4165,  4394,  4583,  4195,  8486,  4165,  4212, 11342,  9589,\n",
       "        4212,  4169,  4162,  6055,  4191,  5592,  7080,  8477,  4165,\n",
       "        7147,  4189,  7200,  4359,  4515,  4165,  4669,  5187,  4865,\n",
       "        4190,  4551,  4448,  4800,  6201,  5247,  6807,  4195,  4349,\n",
       "        5888,  4469,  4782,  4165,  7990,  7080,  4212,  5243,  4216,\n",
       "        4179,    68,  4243,  8876,  4212,  4176,     3,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "train_batches[1][0][3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}