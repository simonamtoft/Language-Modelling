{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports, hyperparameters and other initializations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer and tokenized datasets:\n",
    "tokenizer = Tokenizer.from_file(\"serialized_tokenizer\")\n",
    "train_ds, val_ds, test_ds = load_from_disk(\"tokenized_train\"), load_from_disk(\"tokenized_val\"), load_from_disk(\"tokenized_test\")\n",
    "#train_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "#val_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "#test_ds.set_format(type=\"pt\", columns=[\"ids\", \"attention_mask\"])\n",
    "\n",
    "train_ids = train_ds[\"ids\"]\n",
    "val_ids = val_ds[\"ids\"]\n",
    "test_ids = test_ds[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embed_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            bias = True, # default\n",
    "            batch_first = False, # default\n",
    "            dropout = dropout_rate,\n",
    "            bidirectional = False # default\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    def forward(self, x):\n",
    "        # x: [seq len, batch]\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        # e: [seq len, batch, emb] \n",
    "        _, (h, c) = self.lstm(e)\n",
    "        # h, c: [layers, batch, hidden dim]\n",
    "        return h, c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embed_dim,\n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = n_layers,\n",
    "            bias = True, # default\n",
    "            batch_first = False, # default\n",
    "            dropout = dropout_rate,\n",
    "            bidirectional = False # default\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x, h, c):\n",
    "        # x: [batch], h, c: [n layers, batch, hidden dim]\n",
    "        x = x.view(1, -1)\n",
    "        # x: [1, batch]\n",
    "        e = self.dropout(self.embedding(x))\n",
    "        # e: [1, batch, embed dim]\n",
    "        o, (h, c) = self.lstm(e, (h,c))\n",
    "        # o: [seq len, batch, hidden dim]\n",
    "        p = self.fc_out(o.squeeze(0))\n",
    "        # p: [batch, output dim]\n",
    "        return p, h, c\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        # x: [len, batch]\n",
    "        target_length = target.shape[0]\n",
    "        batch_size = target.shape[1]\n",
    "        output_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(seq_len, batch_size, output_vocab_size)\n",
    "        h, c = self.encoder(x)\n",
    "\n",
    "        # first token\n",
    "        x = target[0, :]\n",
    "        for t in range(1, target_length):\n",
    "            o, h, c = self.decoder(x, h, c)\n",
    "            outputs[t] = o\n",
    "            x = o.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=VOCAB_SIZE, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE)\n",
    "decoder = Decoder(output_dim=VOCAB_SIZE, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(12800, 64)\n    (lstm): LSTM(64, 64, num_layers=2, dropout=0.5)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(12800, 64)\n    (lstm): LSTM(64, 64, num_layers=2, dropout=0.5)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (fc): Linear(in_features=64, out_features=12800, bias=True)\n  )\n)\nParameters: 2,603,520\n"
     ]
    }
   ],
   "source": [
    "def init(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "print(model.apply(init))\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.7)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"[PAD]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #eval stuff\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}